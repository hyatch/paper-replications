This is a replication of a paper published by the Chinese AI team DeepSeek in February of 2025. It is titled "Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling". It is linked here [https://arxiv.org/html/2502.06703v1](url)

As described in their abstract, DeepSeek found a way to beat out massive LLMs with much smaller models in terms of mathematical reasoning using Test-Time Scaling (TTS). DeepSeek revised policy models that can pick out specific reasoning paths an LLM proposes then generating tokens of thought. 
